{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a714b47",
   "metadata": {},
   "source": [
    "### PCA IMPLEMENTATION FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed45395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f99c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4fd735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancerData = load_breast_cancer()\n",
    "X= cancerData[\"data\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba827e",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6bebf6",
   "metadata": {},
   "source": [
    "## Step 1 : Column standarsaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40d992ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = X.mean(axis=0) # calculaye mean column wise\n",
    "X_ = (X-mu)\n",
    "X_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741355db",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20726c31",
   "metadata": {},
   "source": [
    "### Step 2: Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e88b71c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.dot(X_.T,X_)  # S = XT.X\n",
    "S.shape              #It should be 30*30 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d996344",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207128a4",
   "metadata": {},
   "source": [
    "### Step 3: Eigen Values and Eigen Vector / Eigen Values Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c85534c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamdas, vectors = np.linalg.eig(S) \n",
    "vectors.shape\n",
    "# returns w : eigen value's array and v: normalized unit eignen vectors,\n",
    "# shape of w(lamda) is 30*1, all 30 eigen values are stored in 30 different rows\n",
    "# shape of v(eigenVectors) is 30*30. If we want vector v1, then we need to consider column 1 since column wise vectors are stores. So there there are 30 features, then there will be 30*30 vectors\n",
    "# v1 = vectors[:0]\n",
    "# v2 = vectors[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = vectors[:2]\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20f8d81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get top 2 eigen vectors are\n",
    "V = vectors[:,:2]\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c0a83",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b62d5",
   "metadata": {},
   "source": [
    "## Step 4 : Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a4f66a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fe46b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a58c721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.dot(X_,V)  #projection will computed using dot product\n",
    "X_new.shape           # now we will be having 2 features --> 2 columns and 569 samples --> 569 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fea7872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1160.1425737 , -293.91754364],\n",
       "       [1269.12244319,   15.63018184],\n",
       "       [ 995.79388896,   39.15674324],\n",
       "       ...,\n",
       "       [ 314.50175618,   47.55352518],\n",
       "       [1124.85811531,   34.12922497],\n",
       "       [-771.52762188,  -88.64310636]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce5e79b",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d2352",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489bda1",
   "metadata": {},
   "source": [
    "### PCA IMPLEMENTATION USING BUILT IN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ca67876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8605b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c535ee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1160.1425737 , -293.91754364],\n",
       "       [1269.12244319,   15.63018184],\n",
       "       [ 995.79388896,   39.15674324],\n",
       "       ...,\n",
       "       [ 314.50175618,   47.55352518],\n",
       "       [1124.85811531,   34.12922497],\n",
       "       [-771.52762188,  -88.64310636]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit_transform(X) # this dataset obtained from sklearn and X_new dataset obtained in above approach are same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c3cd0",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb0043",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c7e8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6fac97",
   "metadata": {},
   "source": [
    "### CHOOSING THE RIGHT DIMENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e9544",
   "metadata": {},
   "source": [
    "### Method 1 : using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "685d6454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([443782.6051466 ,   7310.10006165])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sklearn provide built-in method called explained_variance_\n",
    "pca.explained_variance_  # this will give lamda1 and lamda2 values. lamda1 =443782.6051466 and lamda2 = 7310.10006165 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83c61ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98204467, 0.01617649])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_ # This will return how much variances are explained/covered by selected features (here 2 features)\n",
    "\n",
    "# Here out of 30 features, first feature/lamda1 explains 98.2% variance and sencond feature explains 1.6% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25ebaec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998221161374173"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total variance coverage considering first two features is : 99.8%\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2f610",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0c40d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f5827",
   "metadata": {},
   "source": [
    "### Method 2 : using algorith built above from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f414cdf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98, 0.02, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we had stores all eigen values in the lamdas variable.\n",
    "# explained variance by lamda1 given by : lamda1/Sum of all 30 lamdas values\n",
    "\n",
    "total = sum(lamdas)\n",
    "explained_variance = np.round(lamdas/total,2)  #This will give the array which has the explained varaince value for all the lamdas\n",
    "explained_variance\n",
    "\n",
    "# lamda1/feature1 --> 0.98 --> 98%\n",
    "# lamda2 --> 0.02 --> 2%\n",
    "# lamda3 --> 0 --> 0%\n",
    "\n",
    "# so in this example considering first two features are enough, becuase rest of the features has 0 variance, so no use of considering them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a0e1500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "consider below sample example \n",
    "\n",
    "lamda1/feature1 --> 0.8 --> 80%\n",
    "lamda2 --> 0.07 --> 7%\n",
    "lamda3 --> 0.05 --> 7%\n",
    "lamda4 --> 0.05 --> 5%\n",
    "\n",
    "In this example, if we consider only first two features, then the variance covered will be only 87%. \n",
    "Remaining 13% variance will be missed. So, to get to know the total number of features required to cover atleast 99%, we can use cummulative sum formula over the lamdas_variance values. \n",
    "i.e., Cummulative sum = lamda1_variance, lamda1_variance + lamda2_variance, lamda1_variance + lamda2_variance + lamda3_variance, lamda1_variance + lamda2_variance + lamda3_variance +lamda4_variance ,.... \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cummulative_variance = np.cumsum(explained_variance)\n",
    "cummulative_variance\n",
    "\n",
    "# here at second position we are getting value 1. ==> 100%, so we should consider first two features to achieve 100% varinace coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc25118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
